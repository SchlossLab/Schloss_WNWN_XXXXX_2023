---
bibliography: references.bib
output:
  pdf_document:
    keep_tex: true
csl: asm.csl
geometry: margin=1.0in
header-includes:
 - \usepackage{upgreek}
 - \usepackage{booktabs}
 - \usepackage{longtable}
 - \usepackage{graphicx}
 - \usepackage{array}
 - \usepackage{multirow}
 - \usepackage{wrapfig}
 - \usepackage{float}
 - \usepackage{colortbl}
 - \usepackage{pdflscape}
 - \usepackage{tabu}
 - \usepackage{threeparttable}
 - \usepackage{threeparttablex}
 - \usepackage[normalem]{ulem}
 - \usepackage{makecell}
 - \usepackage{setspace}
 - \doublespacing
 - \usepackage[left]{lineno}
 - \linenumbers
 - \modulolinenumbers
 - \usepackage{helvet} % Helvetica font
 - \renewcommand*\familydefault{\sfdefault} % Use the sans serif version of the font
 - \usepackage[T1]{fontenc}
 - \usepackage[shortcuts]{extdash}
---


```{r, echo=FALSE}
options(tidyverse.quiet = TRUE)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(ggtext))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(here))
#suppressPackageStartupMessages(library(kableExtra))
suppressPackageStartupMessages(library(glue))

opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("message" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x, digits=2){

  if(is.list(x)){
    x <- unlist(x)
  }
  if(is.numeric(x)){
      paste(format(x,big.mark=',', digits=digits, nsmall=digits, scientific=FALSE))
  } else {
      paste(x)
  }
}
knitr::knit_hooks$set(inline=inline_hook)

package_version <- function(package){

  paste(unlist(packageVersion(package)), collapse='.')

}


oxford_comma <- function(x, digits=2) {

  x <- map_chr(x, inline_hook, digits=digits)

  if(length(x) < 2){
    x
  } else if(length(x) == 2){
    paste(x, collapse = " and ")
  } else {
    paste(paste(x[-length(x)], collapse=", "), x[length(x)], sep=", and ")
  }
}

sim_counts <- read_tsv(here("data/simulated_nseqs_distros.tsv")) 

```

# Waste not, want not: Revisiting the analysis that called rarefaction into question

\vspace{20mm}

**Running title:** Review of "Waste not, want not"

\vspace{20mm}

Patrick D. Schloss${^\dagger}$

\vspace{40mm}

${\dagger}$ To whom corresponsdence should be addressed:


\href{mailto:pschloss@umich.edu}{pschloss@umich.edu}

Department of Microbiology & Immunology

University of Michigan

Ann Arbor, MI 48109

\vspace{20mm}

**Research article**

\newpage

```{r datasets}

```

## Abstract

\newpage

## Introduction

Since the development of sequencing technologies such as those provided by 454 and Illumina, microbiome researchers have struggled to produce a consistent number of sequences from each sample in a dataset. It is common to observe more than 10-fold variation in the number of sequences per sample [XXXXX]. Regardless of the source of this variation, researchers desire approaches to control for uneven sampling effort. Of course, this desire is not unique to microbiome research and is a challenge faced by all community ecologists. Common approaches to controlling uneven sampling efforts have included use of proportional abundance (i.e., relative abundance), normalization of counts, parameter estimation, and rarefaction.

In 2014 Paul McMurdie and Susan Holmes published their "Wast not, want not: why rarefying microbiome data is inadmissible" (WNWN) in PLOS Computational Biology [XXXXX]. This paper has had a significant impact on the approaches that microbiome researchers use to analyze 16S rRNA gene sequence data. According to Google Scholar, this paper has been cited more than 2,300 times as of January 2023. Anecdotely, I have received correspondence from researchers over the past 10 years asking how to address critiques from reviewers who criticize my correspondents' analysis for rarefying (e.g., see [this Twitter thread](https://twitter.com/inanna_nalytica/status/1264679859672006656)). I have also received these types of comments from reviewers, specifically in regards to a preprint that I posted in 20XX in regards to my critique of the practice of removing rare taxa from analyses [XXXXX]. In the process of responding to these critiques and preparing a manuscript investigating rarefaction and other approaches to control for uneven sequencing effort, I decided to reassess the WNWN study including their definitions, simulations, and analyses.

### Confusion regarding what is meant by "rarefying" and "rarefaction"

As I attempted to reproduce the results of WNWN, I noticed that the step that purported to rarefy the data only performed one subsampling of the data (Lines 404 through 416 of `simulation-cluster-accuracy/simulation-cluster-accuracy-server.Rmd`). This caused me to re-inspect how McMurdie and Holmes defined "rarefying" in the following quoted text from their paper:

> Instead, microbiome analysis workflows often begin with an ad hoc library size normalization by random subsampling without replacement, or so-called rarefying [17]–[19]. There is confusion in the literature regarding terminology, and sometimes this normalization approach is conflated with a non-parametric resampling technique — called rarefaction [20], or individual-based taxon re-sampling curves [21] — that can be justified for coverage analysis or species richness estimation in some settings [21], though in other settings it can perform worse than parametric methods [22]. Here we emphasize the distinction between taxon re-sampling curves and normalization by strictly adhering to the terms rarefying or rarefied counts when referring to the normalization procedure, respecting the original definition for rarefaction. Rarefying is most often defined by the following steps [18].
>
> 1. Select a minimum library size, N~L,m~. This has also been called the rarefaction level [17], though we will not use the term here.
> 2. Discard libraries (microbiome samples) that have fewer reads than N~L,m~.
> 3. Subsample the remaining libraries without replacement such that they all have size N~L,m~.
>
> Often N~L,m~ is chosen to be equal to the size of the smallest library that is not considered defective, and the process of identifying defective samples comes with a risk of subjectivity and bias. In many cases researchers have also failed to repeat the random subsampling step (3) or record the pseudorandom number generation seed/process — both of which are essential for reproducibility.

It was unfortunate that McMurdie and Holmes used the term "rarefying" here and throughout their manuscript. The authors were correct to state that the distinction between "rarefying" and "rarefaction" is confusing and leads to their conflation. In my experience, subsequent researchers have conflated the results of this study of the effects of rarefying data with rarefaction of data. As an example, Willis (XXXXX) describes problems with rarefaction rather than rarefying data when citing WNWN in her paper proposing alternatives to rarefaction for use with alpha diversity data:

> Unfortunately, rarefaction is neither justifiable nor necessary, a view framed statistically by McMurdie and Holmes (2014) in the context of comparison of relative abundances.

Adding to the confusion is that the papers cited in the first sentence of the quote I  WNWN included above either do not use the words "rarefy" or "rarefying" or use them interchangably with "rarefaction". In hindsight, as shown in the quoted text, McMurdie and Holmes do emphasize the distinction between rarefying and rarefaction. However, because they seem to have coined a new meaning for rarefying, they seem to have only added to the confusion by using the generally used verb form of rarefaction. Further confusion comes from the author's admonition in the final sentence that some researchers have failed to repeat the subsampling step. To most scientists, repeating the subsampling step is rarefaction. My preference is to use subsampling as the term describing the process they refer to as rarefying. In other words rarefaction with a single randomization.

To provide a more clear definition of rarefaction, I propose the following:

1. Select a minimum library size, N~L,m~. Researchers are encouraged to report the value of N~L,m~.
2. Discard samples that have fewer reads than N~L,m~.
3. Subsample the remaining libraries without replacement such that they all have size N~L,m~.
4. Compute the desired metric (e.g., richness, Shannon diversity, Bray-Curtis distances) using the subsampled data
5. Repeat steps 3 and 4 a large number of iterations (e.g, 100 or 1,000). Researchers are encouraged to report the number of iterations.
6. Compute summary statistics using values generated from the subsampled data

This definition aligns well with how rarefaction was originally defined for comparing richness (i.e., the number of taxa in a community) across communities when communities are sampled to different depths. It is important to note that this procedure generates substantially different results to those obtained without accounting for uneven sampling effort or using relative abundances, normalized counts, compositional data transformatins, variance stabilization procedures, or estimation techniques. I have explored the differences in results obtained using the diversity of approaches for controlling for uneven sampling effort; rarefaction, as described here, outperforms the other approaches [XXXXX].

With this more general approach to rarefaction, rarefaction can be performed using any alpha or beta diversity metric. This strategy has been widely used by my research group and others. The procedure outlined above could also be used for hypothesis tests of differential abundance; however, thought would need to be given to how to synthesize the results of these tests across a large number of replications.

```{r}
gp_counts <- sim_counts %>% filter(simulation == "GlobalPatterns")

gp_n_samples <- nrow(gp_counts)
```

### Description of "Simulation A" from WNWN

McMurdie and Holmes analyzed the effect of rarefying and other approaches on clustering accuracy using what they called "Simulation A" in their Figure 2A and elsewhere in their paper. In Simulation A, they investigated the ability to correctly assign samples to one of two clusters using simulated data used to generate 40 samples from each of two distributions. A variety of approaches were used to calculate distances between the samples those distances were used as input to partitioning around mediods (PAM) clustering with two groups. The accuracy of the cluster assignment was used as the metric to assess performance. This analysis was performed in the `simulation-cluster-accuracy/simulation-cluster-accuracy-server.Rmd` R-flavored markdown file that was published as Protocol S1 in the original paper. Line numbers from this file will be refernced with an "L" as a prefix.

The two distributions were generated using human fecal and ocean data originally take from the GlobalPaterns dataset (L129) [XXXXX]. To generate a fecal and ocean template distribution, the authors included any operational taxonomic unit (OTU) that appeared in more than one of the 4 fecal and 3 ocean samples (L60 and L137). The OTUs were sorted by how many of the 7 samples the OTUs were observed in followed by their total abundance across all 7 samples (L139). From this sorted list they identified the identifiers of the first 2000 OTUs (L66). Returning to the 7 samples they selected the 2000 most common and abundant OTUs and pooled the abundances of the fecal and ocean samples separately to create two templates (L144, L159-160, L197-198).

Next, the fecal and ocean templates were mixed in 8 different fractions to generate two community types that differend by varying effect sizes (1, 1.15, 1.25, 1.5, 1.75, 2, 2.5, and 3.5; L170-195, L220). To simulate the variation in sequencing depth across the 80 samples, they normalized he number of sequences from each of the `gp_n_samples` samples in the GlobalPatterns dataset so that the median number of sequences (N~L~) for the GlobalPatterns had 1,000, 2,000, 5,000, or 10,000 sequences (L324-325). They then randomly sampled the `gp_n_samples` normalized sequencing depths to generate 80 sampling depths. From each community type, they simulated 40 samples by sampling to the desired number of reads (L73, L230-233 and L326-327). Each simulation condition was repeated 5 times (L85). This resulted in 160 simulations (8 effect sizes x 4 median sampling depths x 5 replicates = 160 simulations). Finally, they removed rare and low prevalence OTUs in two steps. First, they removed any OTUs whose total abundance was less than 3 across all 80 samples and that did not appear in at least 3 samples (L368-386). Second, they removed any OTUs that did not have more than 1 sequence in more than 5% of the 80 samples (i.e., 4 samples) and that did not have a total abundance across the 80 samples greater than one half of the number of samples in each community type (i.e., 20) (L523-538, L551).

### Critique of the original simulation design

Although all simulations represent an artificial representation of reality and can be critiqued, ten elements of the design of Simulation A warrant further review. 

1. Simulated conditions were only replicated 5 times each, which caused results to be sensitive to random number generator seed
2. The average sizes of the libraries were small by modern standards
3. A single subsampling of each dataset was evaluated rather than using rarefaction, which likely resulted in noisier data
4. Results using PAM clustering were not directly compared to those of K-means and hierarchical clustering
5. Subsampling removed the smallest 15% of the samples, which penalized accuracy values by 15 percentage points
6. The distribution of library sizes was poorly chosen
7. A filtering step was applied to remove rare taxa from the simulated datasets, which could distort the shape of the communities
8. No test of effect of transformation methods to account for effects of uneven sample size between treatment groups
9. Clustering accuracy was used rather than direct comparisons beta diversity
10. No consideration of effects of transformations on alpha diversity metrics

These points will serve as an outline for the Results section. After replicating the original simulations, these points will be evaluated to reassess whether subsampling or rarefaction are "inadmissible".


## Results

### Replication of WNWN simulations and results

Before assessing the impact of the points I critiqued above, I attempted to replicate the results shown in Figures 4 and 5 of the original paper using the authors original code and my own. I created a Conda environment that used the R version and package versions that were as close as possible to those used in the original paper. Because of the slight differences in packages, it was necessary to apply several patches to the original R-flavored markdown file to get the document to render. I was able to generate a figure similar to that presented as Figures 4 and 5 of the original paper. My results are shown in Figures S2 (**norarefy-source/simulation-cluster-accuracy/Figure_3.pdf**) and S3 (**norarefy-source/simulation-cluster-accuracy/Figure_4.pdf**) of this paper, respectively. The differences in results are likely due to differences in software versions and operating systems. It is also worth noting that their versions of the two figures differ from those included in Protocol S1 within the rendered html file (`simulation-cluster-accuracy/simulation-cluster-accuracy-server.html`) and that the figure numbers are one higher in the paper than those generated by the R-flavored markdown file. Regardless of the differences, the results are qualitatively similar.

### 1. Simulated conditions were only replicated 5 times each

Each simulated condition was replicated 5 times in WNWN and the paper reports the mean and standard deviation of the replicate clustering accuracies. The relatively small number of replicates accounts for the jerkiness of the lines in the original Figures 4 and 5 (e.g. the Bray-Curtis distances calculated on the DEeqVS transformed data). A better approach would have been to use 100 replicates as this would reduce the dependency of the results on the random number generator's seed. Furthermore, because the accuracies are unlikely to be symmetric around a mean at larger accuracy values the median and 95% confidence intervals or intraquartile range should have been reported. To test the effect of increasing the number of replicates, I pulled apart the code in `simulation-cluster-accuracy/simulation-cluster-accuracy-server.Rmd` into individual R and bash scripts that were executed using a Snakemake workflow with the same Conda environment as above. This was necessary since the number of simulated conditions because of this change increased 20-fold. Such intense data processing was not practical within a single R-flavored markdown document. Again, the observed results were qualitiatively similar to those generated using the single R-flavored markdown file (Figures S4 (**pam_subsample15_fig_4.pdf**) and S5(**pam_subsample_fig_5.pdf**) ). The increased number of replications resulted in smoother lines and allowed me to present empirical 95% confidence intervals. For all analyses in the remainder of this paper, I used 100 randomized replicates per condition. 

### 2. The average sizes of the libraries were small by modern standards

In the 10 years since WNWN was published, sequencing technology has advanced and sequence collections have grown considerably. For more modern datasets, it would be reasonable to expect a median number of sequences larger than 10,000 (see Table 1 of [Singleton Paper XXXXXXXX]). Therefore, I included an additional average depth of sampling value of 50,000 sequences with the four average sequencing sampling depths as WNWN (i.e., 1,000, 2,000, 5,000, 10,000). Additional sequencing coverage would be expected to result in more robust distance values since there would be more information represented in the data. Indeed, the added sampling depth showed higher accuracy values at lower effect sizes for the combinations of normalization methods and distance calculations (Figure S4 (**pam_subsample15_fig_4.pdf**)). Increased sequencing coverage also resulted in improved clustering accuracy for lower effect sizes when the library size minimum quantile was decreased (Figure S5 (**pam_subsample_fig_5.pdf**)). I will revisit the choice of the library size minimum quantile below.


### 3. A single subsampling of each dataset was evaluated rather than using rarefaction

As noted above, the original jargon that was used in WNWN was confusing to many who conflated rarefying with rarefaction. One problem with a single subsampling of a community is that it is unlikely to obtain a representative sampling of each community. A more robust analysis would have used rarefaction rather than a single subsampling of the data since it would have averaged across random subsamplings, which individually would be unlikely to represent the overall composition of the communities. Rather than being guilty of "omission of available valid data", rarefaction uses all of the available data. To compare subsampling and rarefaction, I removed the 15% of samples with the lowest number of sequences for each of the 100 simulated datasets and compared the distances from a single subsampling to rarefaction using the distance calculations shown in the original Figure 4. This analysis revealed two benefits of rarefaction. First, the median distances generated by rarefaction was always as large or larger than those from a single subsample (Figure **subsample_rarefaction_compare.pdf**). In general, the difference was most pronounced for smaller average library sizes and at smaller effect sizes. The unweighted UniFrac distances were most impacted by the use of rarefaction over subsampling. Second, the intraquartile ranges for the distances generated by rarefaction were generally smaller than those by subsampling and showed similar trends to the difference in the median distances (Figure **subsample_rarefaction_compare.pdf**). The intraquartile ranges for Bray-Curtis, Euclidean, and unweighted UniFrac distances were actually larger by rarefaction than by subsampling at small effect sizes and average library sizes; however at larger values the intraquartile range by subsampling was larger than by rarefaction for these distance calculations. Because rarefaction incorporates more of the data and generally performed better than subsampling, the remainder of this analysis will report results using rarefaction rather than by subsampling except when noted.


```{r}
cluster_data <- read_tsv(here("data/simulation_clusters.tsv.gz")) %>%
  filter(simulation == "sim_a") %>%
  filter(filter == "filter") %>%
  filter(fraction != "s1" & fraction != "1") %>%
  filter((distance == "bray" &
            transform %in% c("proportion", "subsample15", "rarefaction15", "none", "deseq")) |
        (distance == "euclidean" &
            transform %in% c("none", "subsample15", "deseq", "rarefaction15")) |
        (distance == "poisson" &
            transform %in% c("rarefaction15", "subsample15", "none")) |
        (distance == "logFC" &
            transform %in% c("upperquartile", "subsample15", "rarefaction15")) |
        (distance == "uunifrac" &
            transform %in% c("rarefaction15", "subsample15", "proportion")) |
        (distance == "wunifrac" &
            transform %in% c("rarefaction15", "subsample15", "none", "proportion", "deseq"))
        ) %>%
  filter(fraction == 1.15) %>%
  group_by(n_seqs, transform, distance, rep) %>%
  slice_max(fracCorrect) %>%
  ungroup() %>%
  mutate(fracCorrect = 1) %>%
  select(n_seqs, rep, transform, distance, method, fracCorrect) %>%
  pivot_wider(names_from = method, values_from = fracCorrect, values_fill = 0)

best_method <- cluster_data %>%
  filter(transform != "subsample15") %>%
  summarize(pam = 100 * mean(pam),
            kmeans = 100 * mean(kmeans),
            hclust = 100 * mean(hclust))

rarefaction <- cluster_data %>%
  filter(transform == "rarefaction15") %>%
  group_by(n_seqs, distance) %>%
  summarize(pam = 100 * mean(pam),
          kmeans = 100 * mean(kmeans),
          hclust = 100 * mean(hclust),
          k_best = kmeans >= pam & kmeans >= hclust, .groups = "drop")
  
rarefaction_count <- rarefaction %>%
  count(k_best) %>%
  pivot_wider(names_from = k_best, values_from = n)

stopifnot(sum(rarefaction_count) == rarefaction_count$`TRUE`)


subsample <- cluster_data %>%
  filter(transform == "subsample15") %>%
  group_by(n_seqs, distance) %>%
  summarize(pam = 100 * mean(pam),
          kmeans = 100 * mean(kmeans),
          hclust = 100 * mean(hclust),
          k_best = kmeans >= pam & kmeans >= hclust, .groups = "drop")
  
subsample_count <- subsample %>%
  count(k_best) %>%
  pivot_wider(names_from = k_best, values_from = n)

pretty_distances <- c(bray = "Bray-Curtis",
                      euclidean = "Euclidean",
                      poisson = "Poisson",
                      logFC = "Mean Squared Difference",
                      uunifrac = "Unweighted UniFrac",
                      wunifrac = "Weighted UniFrac")

fails <- subsample %>%
  filter(!k_best) %>%
  mutate(distance = pretty_distances[distance])
```

### 4. Results using PAM clustering were not directly compared to those of K-means and hierarchical clustering

The clustering accuracy measurements in the body of the manuscript were determined using PAM-based clusters while Protocol S1 also includes K-means and hierarchichal clustering. Although the data were not displayed in a manner that lent itself to direct comparison, close inspection of the rendered figures suggests that PAM may not have been the optimal choice in all situations. Rather, K-means clustering may have been prefered. Because the accuracies were the smallest at lower effect sizes, I focused my comparison at the effect size of 1.15. For each set of 100 replicated simulated datasets, I compared the clustering accuracy across clustering methods to see how often each clustering method resulted in the highest accuracy Figure **compare_cluster_methods.pdf**). Indeed, K-means clustering performed better than the other methods. Among all combinations of normalization methods, distance calculations, and read depths, PAM clustering resulted in clustering accuracies as good or better than the other methods in `r best_method$pam`% of the randomizations (Figure **compare_cluster_methods.pdf**). K-means clustering was at least as good as the other methods in `r best_method$kmeans`% of the randomizations. HClust was at least as good as the other methods for `r best_method$hclust`% of the randomizations. I specifically compared the clustering accuracies using rarefaction for each of the distance calculations methods using PAM and K-means clustering. Among the `r sum(rarefaction_count)` combinations of distance calculations and read depths, K-means clustering performed better than PAM in each case. Even when using subsampled data, K-means performed better than PAM in `r subsample_count[["TRUE"]]` cases with PAM doing better in `r subsample_count[["FALSE"]]` cases when calculating distances with `r oxford_comma(unique(fails$distance))` using `r oxford_comma(format(fails$n_seqs, big.mark = ","))` sequences. Because K-means clustering did so much better than PAM clustering in the simulated conditions, I will use K-means clustering for the remainder of this study.

```{r}
clusters <- read_tsv(here("data/simulation_clusters.tsv.gz")) %>%
  filter(simulation == "sim_a") %>%
  filter(filter == "filter") %>%
  filter(method == "kmeans") %>%
  filter(distance != "bcv") %>%
  filter(str_detect(transform, "rarefaction")) %>%
  filter(fraction != "s1" & fraction != "1") %>%
  select(fraction, n_seqs, rep, transform, distance, method, fracCorrect) %>%
  group_by(fraction, n_seqs, transform, distance, method) %>%
  summarize(median = median(fracCorrect),
            lci = quantile(fracCorrect, 0.025),
            uci = quantile(fracCorrect, 0.975), .groups = "drop") %>%
  mutate(quant = as.numeric(str_replace(transform, "rarefaction", "")),
        distance = pretty_distances[distance]) %>%
  select(fraction, n_seqs, distance, quant, median)

not_zero <- clusters %>%
  group_by(fraction, n_seqs, distance) %>%
  arrange(quant) %>%
  slice_max(median, with_ties = FALSE, n = 1) %>%
  filter(n_seqs >= 2000 & quant != 0) %>%
  pull(distance) %>%
  unique()
```

### 5. Subsampling removed the smallest 15% of the samples

In WNWN, the authors quantified the tradeoff between sampling effort, the number of samples, and clustering accuracy (original Figure 5, my Figure S5). Although the optimal sampling effort varied by distance metric, transformation method, and sampling effort, they removed samples whose number of sequences was less than the 15th percentile (L404-419). They acknowledged that this screening step, which was only used with subsampling, would decrease clustering accuracy putting it at a relative disadvantage to the other methods (page 5, column 1, last paragraph). Therefore, it was not surprising that the peak clustering accuracy for their subsampled data was at 85% (Figure S2). Because the true result would not known *a priori* in microbiome studies, it would be impossible for researchers to conduct a sensitivity analysis comparing the tradeoffs between sequencing depth, sample number, and clustering accuracy to select a sampling depth. [**Although the authors claim that "Rarefying counts requires an arbitrary selection of a library size minimum that affects downstream inference" (page 8, column 1, point 3), in acutal microbiome studies the selection of a sampling depth is not as arbitrary as the authors claim. Rather, to avoid p-hacking, researchers pick a set of criteria where they will include or exclude samples prior to testing their data.**] The differences in clustering accuracy between subsampling and rarefaction and using PAM and K-means clustering indicated that it was necessary to reassess the tradeoff between the library size minimum quantile and clustering accuracy. When using rarefaction and K-means clustering, the only distance calculations that did not have their maximal clustering accuracy when using the full dataset were `r oxford_comma(not_zero_calcs)` (**kmeans_rarefaction_fig_5.pdf**). These results showed that for modern sequencing depths, using the full datasets with rarefaction and K-means clustering resulted in accuracies that were typically better than those observed when removing the smallest 15% of the samples from each simulated dataset. When the original Figure 4 was recast with these approaches, Rarefaction performed at least as well as any of the other tranformations with each distance calculation, except for with the Poisson distance (Figure **kmeans_rarefaction15_fig_4.pdf**). It is worth noting that at larger effect sizes, K-means clustering did not perform as well for some combinations of normalization methods and distance calculations (compare **kmeans_rarefaction15_fig_4.pdf** and **pam_subsample00_fig_4.pdf**); however, those combinations that performed worse by K-means were not as good as rarefaction or subsampling by either clustering method.


```{r}
gp_median <- median(gp_counts$n_seqs)
gp_mean <- mean(gp_counts$n_seqs)
gp_sd <- sd(gp_counts$n_seqs)
gp_min <- min(gp_counts$n_seqs)
gp_max <- max(gp_counts$n_seqs)


log_counts <- sim_counts %>%
  filter(simulation == "Log-scaled")

log_median <- median(log_counts$n_seqs)
log_mean <- mean(log_counts$n_seqs)
```
### 6. The distribution of library sizes was poorly chosen

As described above, the sequencing depths used in the `r gp_n_samples` GlobalPatterns datasets were used as the distribution to create sequencing depths for the 80 samples that were generated in each simulation. The GlobalPatterns datasets had a mean of `r gp_mean` sequences and a median of `r gp_median` sequences per dataset. The datasets ranged in sequencing depth between `r gp_min` and `r gp_max` sequences for a `r gp_max/gp_min`-fold difference. Rather than representing a typically observed distribution of sequencing depths that would be skewed right, the sampling distribution was normally distributed (Shapiro-Wilk test of normality, P=`r shapiro.test(gp_counts$n_seqs)$p.value`) (Figure ***distribution_shape.pdf***). From these simulations it is unclear how sensitive the various transformations and distance calculations are to a skewed distribution. A second limitation of this sampling distribution is that it only contained `r gp_n_samples` unique sampling depths such that each sampling depth would have been re-used an average of `r 80/gp_n_samples` times in each simulation. Yet, it is unlikely for a real sequence collection to have duplicate sequencing depths. To reassess the WNWN results in the context of a more typical distribution of sample sizesI created a new set of simulations that would focus on solely on the effect of the shape of the distribution on the results. The issue of number of samples and the distribution of their sequencing depths in the context of controlling for uneven sampling effort is explored in far greater detail in an another analysis [XXXXXX]. 

I created a simple sequencing depth distribution where there were 80 depths logarithmically distributed between the minimum and maximum sequencing depths of the GlobalPatterns dataset (Figure ***distribution_shape.pdf***). The resulting median of this distribution was `r log_median` and the mean was `r log_mean`.

Generate new Figure 4 and 5 w/ kmeans


### 7. A filtering step was applied to remove rare taxa from the simulated datasets, which could distort the shape of the communities

McMurdie and Holmes were emphatic that "**rarefying biological count data is statistically inadmissible** because it requires the omission of available valid data" (emphasis in original). Thus it is strange that they argue against removing data when rarefying/subsampling, but accept removing rare and low-prevalence OTUs prior to normalizing and analyzing their simulated communities. This practice has become common in microbiome studies and is the standard approach in tools such as dada2, unoise, and deblur [XXXXXXXX]. However, my previous work has shown that rare sequences from a poorly sequenced sample often appear in more deeply sequened samples suggesting that they are not necessarily artifacts. Furthermore, removing rare sequences alters the structure of communities and has undesirable effects on downstream analyses [XXXXXXXX]. 

Filtering or not doesn't impact clustering accuracy with the GlobalPatterns distribution since filtering doesn't remove much - likely because the same sample size is used multiple times within a treatment group... Still rarefaction/subsample more stable that others - deseq, uq, proportion, none

```{r}
clusters <- read_tsv(here("data/simulation_clusters.tsv.gz"))

clusters %>%
  filter(simulation == "sim_a") %>%
  filter((distance == "bray" &
            transform %in% c("proportion", "subsample15", "rarefaction00", "none", "deseq")) |
        (distance == "euclidean" &
            transform %in% c("none", "rarefaction00", "deseq", "rarefaction15")) |
        (distance == "poisson" &
            transform %in% c("rarefaction00", "subsample15", "none")) |
        (distance == "logFC" &
            transform %in% c("upperquartile", "subsample15", "rarefaction00")) |
        (distance == "uunifrac" &
            transform %in% c("rarefaction00", "subsample15", "proportion")) |
        (distance == "wunifrac" &
            transform %in% c("rarefaction00", "subsample15", "none", "proportion", "deseq"))
        ) %>%
  filter(fraction != "1" & fraction != "s1") %>%
  filter(method == "kmeans") %>%
  select(-fracCorrectPred) %>%
  pivot_wider(names_from = filter, values_from = fracCorrect) %>%
  mutate(f_nf = filter - nofilter) %>%
  group_by(fraction, n_seqs, transform, distance) %>%
  summarize(mean = mean(f_nf), .groups = "drop") %>%
  ggplot(aes(x = as.numeric(fraction), y = mean, group = transform, color = transform)) +
  geom_line() +
  facet_grid(n_seqs ~ distance)
ggsave("test.pdf")
```


### 8. No test of effect of transformation methods to account for effects of uneven sample size between treatment groups

In previous analyses I have seen that removing rare taxa and not rarefying data can lead to falsely detecing differences between communities when sampling effort is confounded with the treatment group [XXXXXXX]. For example, such situations have been observed when comparing communities at different body parts where one site is more likely to generate contaminating sequence reads from the host. To see if this result was replicated with the WNWN framework, I used by both the GlobalPatterns and log-distributed sequence distributions but skewed the counts so that the samples with sequencing effort below and above the median were in separate treatment groups. 

For the case when the effect size was 1.0, the samples should have only been assigned to one cluster. Thus the clustering accuracies shown for that effect size in the original Figure 4 are misleading. When there are two groups of 40 samples that do not differ, the best a method could do would be to correctly assign 41 of the 80 samples for an accuracy of `r 41/80`.

### 9. Clustering accuracy was used rather than direct comparisons beta diversity

The analysis of Simulation A in WNWN used clustering accuracy to assess the effect of different normalization procedures, distance calculations, and clustering methods. There has been controversy over the meaning of clustering samples (i.e., enterotypes) including whether such clustering should be done on ecological distances or sequence counts and the biological interpretation of such clusters [XXXXXXXXX]. One notable challenge with using clustering accuracy as the dependent variable is that the clustering methods force the samples into one of two clusters. For the case where the effect size was 1.0, it was impossible for all 80 samples to be assigned to a single cluster. As has already been demonstrated, an additional challenge with using clustering accuracy is that there is sensitivity to the clustering method that a researcher selects. Given the apparent interaction between the various variables and clustering method with clustering accuracy, an alternative approach is needed. In fact, a more common analysis of distance matrices would be to use a non-parametric analysis of variance test of the various distance matrices. Therefore, I subjected each of the distance matrices to such a test using `adonis2`, a function from the vegan R package that implements this test. This analysis allowed me to quantify the probability of falsely rejecting a null hypothesis (i.e., Type I error) when the effect size was 1.0 and the power to detect a difference when the effect sizes were larger than 1.0. Such an analysis was not possible in the original study since only 5 replicates of each simulation were performed. 

Type I error...


Type II error...


### 10. No consideration of effects of transformations on alpha diversity metrics

Furthermore, a comparison of alpha diversity metrics could be made to assess the impact of normalization procedures on measurements such as richness and diversity. 




## Discussion

* Gratidue that code was published with paper. This made it straight forward to notice that only a single subsampling step was performed for each random seed and that only 3 random seeds were used.
* Differential abundance issues
* How we actually pick a threshold

## Acknowledgements

\newpage

## References

\setlength{\parindent}{-0.25in}
\setlength{\leftskip}{0.25in}
\noindent

<div id="refs"></div>
\bibliography{ref}
\setlength{\parindent}{0in}
\setlength{\leftskip}{0in}

\newpage

## Figures


